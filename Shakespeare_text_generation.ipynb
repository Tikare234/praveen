{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOgMqMLaqGxYbb5vq66h4rV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tikare234/praveen/blob/master/Shakespeare_text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n"
      ],
      "metadata": {
        "id": "4xVWm-NFXgXT"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "5IYY3ackXqvz"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtwSqnMxX74i",
        "outputId": "71005162-10f1-4231-d6cc-dda1102b1ce0"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILo9l5L_YGFk",
        "outputId": "c049c524-a25e-4872-eb87-cfc9b979802c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLXKGkxPYQkT",
        "outputId": "f8236d43-1833-45e7-a976-cb132b70c1f2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZYpf-tkYZvz",
        "outputId": "573f29a7-3524-4da5-cfee-17508ce1a3ae"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "XcfqSfS9ay9x"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIHkt5nKcMoM",
        "outputId": "11325b75-a7c5-4de5-a428-0624eccb7974"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "pXvA0D1NcXqx"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xk_OTEqQiT1-",
        "outputId": "52e965d0-af15-44c5-bd70-06a0c0f9237f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RGYJ_duiVUQ",
        "outputId": "bd861342-f412-4506-9945-04978f5532c7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "JHzWUfbJnWY4"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhPG6GXHoScX",
        "outputId": "3c99dc00-0067-4c60-8670-8e6b39301724"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "duLFp2o-pAUo"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8enk9a8pEBx",
        "outputId": "61e3e368-30f1-41f2-f9a0-250c13f393bd"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "4kF0rQvcVUFd"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0tfEy0ZWZr7",
        "outputId": "340d61e1-de4b-4720-f752-ebd735db4e26"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBgGmWAVVUJH",
        "outputId": "68acc96e-3204-4ef3-9eac-d020e29db60f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "n5NmJ-1JVUMp"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w3U2Lb_WbHy",
        "outputId": "4e14a864-fea7-4ac2-f83f-dfc96f8d63af"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "onLz4iEnWbPs"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Rxs99NEWnmr",
        "outputId": "d7948d72-3a72-432d-827b-8cf8c444f19b"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzYQAg5DWqp8",
        "outputId": "b52ee732-b1d2-4738-de43-d922b91edecb"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "psvKauuYWuKs"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "YE94jwc9WwRF"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "AcFo1bnmW6_J"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCO8M3v0W97o",
        "outputId": "76478a62-8322-41b0-bc48-a9ce800bcb92"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWHNJ9_ZYFmT",
        "outputId": "9bef7ecd-0fb9-4c2a-d537-11b24087d19c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     multiple                  16896     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "1KruXlNiYFu2"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSYFfPOBYUdI",
        "outputId": "2f6f78a4-54e3-45a6-82b4-c975d21d543f"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([16, 32, 48, 41, 53, 13, 48, 47, 61, 18, 16, 17, 46,  7,  7, 11,  5,\n",
              "       32, 31, 37,  4,  4, 57, 65, 57, 61,  9, 32,  3, 40, 64, 28, 20, 28,\n",
              "       20, 61, 17, 24, 18, 17, 32, 48, 64, 56, 10, 36, 18, 47, 46, 18, 21,\n",
              "       25, 64, 44, 34,  1, 42, 52, 10, 40,  3, 25, 18, 22, 57, 36, 47, 11,\n",
              "       57, 30, 11, 19, 32, 29, 34, 46, 60,  7, 12, 57, 12, 44, 51,  7, 13,\n",
              "       36,  5, 35, 39, 24, 25, 42, 36, 42, 13,  1, 15, 60, 18, 19])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1czv6Xsb99C",
        "outputId": "a27a5c5c-b89b-4b68-de79-9b9d314d4a3b"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b' report.\\n\\nJULIET:\\nThat is no slander, sir, which is a truth;\\nAnd what I spake, I spake it to my face'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'CSibn?ihvECDg,,:&SRX$$rzrv.S!ayOGOGvDKEDSiyq3WEhgEHLyeU\\ncm3a!LEIrWh:rQ:FSPUgu,;r;el,?W&VZKLcWc?\\nBuEF'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "sIxzYMPmb-FQ"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg_8x8IAcDSK",
        "outputId": "51a896d8-61aa-4b9d-ea75-2296fb622d30"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1903954, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUGnaJMwcDV5",
        "outputId": "0045104f-2d12-498c-d022-70138c6d2642"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.0489"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JGOFx7n5hhiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "JC8nk8iacDYy"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "KioPcS--cJO6"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "92WyYoEXcJaL"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCzpZGF5cOdt",
        "outputId": "7592f998-42ca-4c50-a87e-1ac24207fe3b"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 14s 56ms/step - loss: 2.7243\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 11s 51ms/step - loss: 1.9961\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 11s 51ms/step - loss: 1.7233\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.5621\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 12s 52ms/step - loss: 1.4626\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.3923\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.3394\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.2956\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.2551\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.2152\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 11s 52ms/step - loss: 1.1786\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.1366\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.0959\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.0514\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.0048\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 54ms/step - loss: 0.9562\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 0.9032\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 0.8519\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 0.7987\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.7491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "eVs61hl2cOhK"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "bZ22BQcjf3n2"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waKxdTEjf3ve",
        "outputId": "f5e238ce-296d-4bd0-dcd6-9e2af6606312"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Harry of Hereford, Lend me and be ascubled\n",
            "Untold, unbroking-fawns-ellaged so: and wilt thou be made,\n",
            "Was it not to chide at him.\n",
            "\n",
            "PARIS:\n",
            "Go, good Montague, betring 'Shined, you\n",
            "have not so cledity: if she be our counsel\n",
            "eyes, when he had changed into a strange eye,\n",
            "Who was report to hid his full and line of them;\n",
            "What is thy news, as she's able, they were at Caius Marcius\n",
            "A season distruction: banished\n",
            "The metean right royal behalf;\n",
            "And, if a cur down beloved, and sure,\n",
            "As beauty use well open'd young moves here?\n",
            "\n",
            "VALERIA:\n",
            "It dotes,--\n",
            "\n",
            "MIRANDA:\n",
            "Where, by those gallants,\n",
            "Mistress must be a Richard since thou art.\n",
            "\n",
            "GLOUCESTER:\n",
            "I thought or honour, but with him times before,\n",
            "And made dread spite and hull forget, to their causes\n",
            "Which makes them makes them against the vester.\n",
            "\n",
            "QUEEN:\n",
            "'Tis beauteful tedious, it groans,\n",
            "Ratcliff, my husband, I hold it there.\n",
            "\n",
            "PETER:\n",
            "And so was, I would to God, sir, well, I have quenched him.\n",
            "\n",
            "TRANTIO:\n",
            "'Tis more, From fitty.\n",
            "\n",
            "CAPULET:\n",
            "Ambleth these gentleme \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.743337631225586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkYlUfAaf3yz",
        "outputId": "32d8759f-3dc8-486e-ec77-a68727593d56"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nThou art a too much less, and for more gracious and these\\ngreat very heart, take in her gown, a gentlewoman,\\nIf he had sworn to sleep so far assail,\\nAnd some revelless cousur'd witch Salors, Henry, both you\\nShow me but sometime hence. Then for thee,\\nHold Harryine will have God in thee Barner's\\nWithollow'd in the one direct barren to thyself.\\nThen wast thou in such careful son,\\nWhich stands so track, as his fools,\\nA hundred years, and a smell-horse is full of\\nenjoy! wherefore are gentle livers.\\n\\nMENENIUS:\\nYou have patient tells;\\nAnd by a minute of our commandment,\\nI mean insolent drop: he fought it;\\nHe was to ten two stoof; and every man may know\\nWhat sensible of Calibans\\nWith oppories impression of Juliet?\\nSee, what am I, sir? that, as I intend to do.\\n\\nKING EDWARD IV:\\nTrue arm me, thy good pardon, I thought Claudio\\nlevy in your mistress. Thou hast\\nYour friends to the service, I'll warm thy parse. What, you news\\nUpon your prantician than hastily exercise,\\nAnd through the bore before yo\"\n",
            " b\"ROMEO:\\nI think there is no more to sign;\\nTo the prince forthwith, nor the heart to teel;\\nBut sent ugnayling at your equall'd,\\nBe whose being allowing to paise ut,ild there.\\n\\nCAMILLO:\\nI may not wet; I mean, an I am no\\nmister's exile. For that had rather was her made before\\nhimself, as the Basknable: well, here himself and lost\\nWith thy deliverance with clouds open'd, nature,\\nOf thy hoon, which, being a bawd? What, Dide, Say.\\n\\nNurse:\\nRight of younce with me intouch a mad man shall be tender-need!\\nThou, wilt thou not, sin? Or Paris a noble hands\\nOf sorrow had thou must diserse thee with my true troop\\nTo give you brown and will show yeur orn,\\nSignior Baptista, govern'd hours!\\nLook where he is a goodly gentleman!\\n\\nHASTINGS:\\nI thank to weephy: and therefore hence away two yourself:\\n'We shall be younged with your will command.\\nUnder your own desire and the man, who\\nAngrated that King Henry, and his son\\nWas wonder'd that which should be compare to\\nthee,--For the King of Norfolk sent.\\n\\nWARWICK:\\nSo, l\"\n",
            " b\"ROMEO:\\n\\nKING EDWARD IV:\\nSo then degrales it troubled the sea, brown\\nDesire the feat of Mantua.\\n\\nBAPTISTA:\\nTake him his bed: therefore frown a man may stand up, you\\nshall have, men and thee: good friar, wherefore discovery\\nOn my shroud lords, beseech your house:\\nThough thyself favour, I make reclisted\\nA thing of war, no sign.\\n\\nHORTENSIO:\\nSir, come pregade her! I am sprit and give\\nHis death resisters to my behossolant\\nAnd Watchmonder-recemberitions, whilst thou\\nwert to be amist it, that they sue plain\\nThe tedions in't; and therefore we lose.\\nFather, son: do you not, my lords, to smile.\\nI see thee stand to if: your grace am ready.\\n'Tis time to soon their words dwall becomes you:\\nI have good name come to hell where Milan\\nSeveral to his good exence?\\n\\nHENRY BOLINGBROKE:\\nI know, sit, I am said that shall come over the\\ncourt.\\n\\nClown:\\nThank you. Twick betrution's for tailor highness is sine\\nAs old save their fresh-time remorse:\\nAll this of hence man that I may sunce\\nhence, Cominius speak.\\n\\nANGELO:\\nWh\"\n",
            " b\"ROMEO:\\nThat thou hast allian and flattering wit our noble\\nAnd fill the prince, have vantage bestilous\\nHad had thought our same replenies? is\\nhe will commended against his folling ears,\\nUnless to unaccust me nothing else.\\n\\nSICINIUS:\\nLet's hence his Ammition.\\n\\nDUKE VINCENTIO:\\nSo that it was?\\n\\nLADY CAPULET:\\nNay, lot any join with scruples are they need.\\n\\nLADY ANNE:\\nHe'll stay with Cityfflance and this blood\\nHath strew'd, and so did then before him.\\n\\nCOMINIUS:\\nThere's some unsight newly demitant.\\n\\nALONSO:\\nPrithee now, I see thee never means.\\n\\nMENENIUS:\\nTake this falinal, sir.\\n\\nCATES:\\nMy liege, my lord; I do not.\\n\\nNurse:\\nIt is my sight! is a very like a shame,\\nWhen such bad men sure of thy face,--\\n\\nMENENIUS:\\nGood gentleman!\\n\\nOXFORD:\\nAwhile arm fail of it. So he within;\\nHe is lost that born here, my noble mother,\\nAnd thou a plague o' doth take age and\\ncalled me but enough to make thee, or smoved me at\\nCall him his name and musty could not know that he had friend;\\nAnd by their other senses smalls; \"\n",
            " b\"ROMEO:\\nI pray you, sir, he had soonest it out.\\nNever trust to this, and e'er a graver\\nAufidius will answer to help you.\\n\\nPETRUCHIO:\\nWhy, madam, shall I find thee mean-spear. Is he or how?\\n\\nBAPTISTA:\\nHus adain,--\\nThe worthy seven emperial hand the words\\nTo whom by thriveing griefs that is banish'd: therein\\nClaudio's watch congenes, and they come with willing.\\nI mean, with fixe your borns; but of whence I shall be from your thies,\\nAnd favour'd hours of my own proceedings, herein,\\nBy heaven, my sepulchre, where we no more words\\nWhich you are, honesty I never seen.\\n\\nSICINIUS:\\nAy, thus mercy is well.\\n\\nHASTINGS:\\nI thank thee, for I said of this: you need noise,\\nMy fortune to account my coap-able to\\nmore uncless not. I late not of necessity,\\nWhich issued foul wife and a thing of mine?\\n\\nKING EDWARD IV:\\nWhy, very well-bragh! how far off with impridition, lip\\nThe fure of ily bull: sue, and queens;\\nAnd, therefore hanging his heir, my own brothers both\\nTo the service of you and woe sound--\\nWhite I to Cl\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.9478161334991455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDZ7RVu_f32Z",
        "outputId": "a1494827-6cc8-4373-d53e-30de7f7eb2d6"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7b221ff1f0a0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jrrCJNuf35F",
        "outputId": "b23a2b2a-6469-405b-eece-406ff022dd8f"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Well, let her ersu between as best well\n",
            "Speak good news! wold Sir Jirn on He,\n",
            "As he good compasion \n"
          ]
        }
      ]
    }
  ]
}